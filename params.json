{"name":"Bayesian-asians-project","tagline":"Data Science - CS109 Project","body":"### Where it all began\r\n\r\n<img src=\"images/intro_image.jpg\" alt=\"intro_image\" class=\"inline\"/>\r\n\r\nMusic is a huge part of our lives - from popular radio stations to our favorite artists, listening to music has not only become a source of relaxation and pleasure but a passion. We wanted to explore this area and dissect what makes some music, songs/artists more popular than others, to be able to predict the popularity of a song. \r\n\r\n<img src=\"images/echonestlogo.jpeg\" alt=\"echonestlogo\" class=\"inline\"/>\r\n\r\nThe source of our data is an API service called Echo Nest. They provide a wide coverage of data on detailed song information from millions of artists. Because of the diverse feature set they provide, we decided their data would best suit our inquiry as to what aspects of songs most strongly affect their popularity. \r\n\r\n### Project Goals\r\n\r\nWe wanted to be able to predict the popularity of a song from its intrinsic aspects - that is, are songs with fast tempos more likely to be popular? Put together, do all these \"popularity-inducing\" aspects of a song make a song popular? Or is there something more in the picture? \r\n\r\nTo that end, we assessed our performance by whether or not we could predict whether a song made it to the Billboard Top 100. To that end, we wanted to measure how features such as tempo, song length, lyrics, genre, energy, and much more contributed to popularity on a national level. \r\n\r\n### Taking a Look at Our Data\r\n\r\nWhen we attempted to take an initial look at our data, we found some interesting results. The genre distribution, for instance, was heavily skewed towards studio, vocal, and electric categories. \r\n\r\n<img src=\"images/song_genres.jpeg\" alt=\"song_genres\" class=\"inline\"/>\r\n\r\nWhen we looked at the feature distribution, we mostly found normal distributions, nothing too crazy. Unfortunately, when we did a pairwise correlation plot between the features and colored the Top 100 different, we found that it was very difficult to distinguish the data points. This will have to be something we keep in mind moving forward. \r\n\r\nTODO: Add in stuff here, IF WE HAVE TIME\r\n\r\n### Initial Modelling\r\n\r\nBecause of the way we composed it, our dataset inherently consists of a small minority of songs (7.5%) that were in the Billboard 100 and the vast majority of the artists' other songs that did not make it to the Billboard 100. Interestingly enough, this suggests on average one song out of each album of a famous artist actually makes it into the Billboard 100. The question we ask is what characteristics determine that one song out of the album? In trying to predict which songs will be in the Billboard 100, we can at least establish a baseline \"No\" model of deeming every song not worthy of getting into the Billboard 100 since so few of them do.\r\n\r\nThe first classifier we used to predict which songs were Billboard 100 material was a Logistic Regression model. We first split our dataset into testing and training datasets and then trained two separate models using L1 (Lasso) and L2 (Ridge) regularization to penalize and minimize contributions of additional variables to prevent overfitting. However, we seemed to be making the same decisions as a baseline \"No\" model and barely showed any improvement in differentiating between true positives and false positives with an area of 0.53 and 0.57 beneath the ROC curve for Lasso and Ridge respectively.\r\n\r\nSimilarly, we attempted a Linear SVM. Unfortunately, as we noted from doing pre-analysis visualization of the song features, our features aren't easily linearly separable; what this entails is that finding a decision boundary by which to distinguish songs in the top 100 from songs not in the top 100. When we attempted this method, we found that it performed a little better than Logistic Regression but barely better than baseline with area beneath the ROC curve of 0.57.\r\n\r\nOur best result was from a Random Forest classifier where the area under the ROC curve was 0.66, greater than both Logistic Regressions and the Linear SVM. It appeared that the decision tree model was able to somewhat differentiate Billboard 100 worthy and not-worthy songs based on the features we had in the data, or was at least attempting to do so since the confusion matrix was the only one to involve true negatives and false negatives. Still, the Random Forest classifier had a pretty low success rate and we wondered if there other techniques we could use to improve our models further.\r\n\r\nOne way we thought we could improve model success rate was through better feature selection. Using a score function based on pearson correlations, we attempted to select features that correlated highly with the response but not with each other. Thus, we hoped to build a model that could explain what features of songs were associated with Billboard 100 success, but didn't overfit. Unfortunately, training an SVM through Pearson pipeline feature selection didn't seem to improve our ROC curve scores.\r\n\r\nIn addition, we noticed our dataset was wildly asymmetric and skewed towards negative responses by default. Thus, we decided to subset our training data so it had the same amount of negative and positive responses, creating a balanced dataset. After training an SVM on this balanced dataset, we noticed it didn't score higher than the SVM trained on the asymmetric training dataset.\r\n\r\nIn conclusion, Random Forest performed the best out of our binary classifiers, with SVM and Logistic Ridge doing better than Logistic Lasso. Feature selection did not seem to improve performance of the classifiers, perhaps because there weren't many high correlating features to Billboard 100. Unfortunately, training models on a balanced dataset also did not significantly improve performance.\r\n\r\n### Adding Lyrics to the Equation\r\n\r\nOur goal for this section was to generate a separate predictor that would allow us to better refine our model. We were of the opinion that songs were a little bit different from the reviews that we have analyzed in the past. While a specific song may have a topic, we believed that it wouldn't be helpful because songs that sing about many different things will make it into the Billboard Top 100 in any given year.\r\n\r\nAs a result, the other option that we had was to use the song lyrics directly and attempt to train a linear model on them. We thought that this would be an especially good idea for songs. Often times in other documents, the frequency of a word doesn't matter as much. However, with a song, a frequent word is usually used for repetition, so such a model, in theory, should work especially well for the problem at hand. After training a Lasso model on the data directly, we found that the words that worked best were a combination of nostalgic and angry words.\r\n\r\n<img src=\"images/word_map.jpeg\" alt=\"wordmap\" class=\"inline\"/>\r\n\r\nAfter generating these results, we then created a predictor using them. For each song, we used the linear model and simply added together all of the values that the words in the songs had. This created another predictor for us to use in the final models.\r\n\r\n### Our Results\r\n\r\nThis project highlighted a few important topics, the first being that predicting billboard results is a very, very difficult task - there is a reason why people can't even predict who will be the next popular artist. The second was that often, despite much of the knowledge that we've learned about how to improve results in our models, it is difficult for a real-life dataset to also have the same kinds of improvements as theorized. The best example of this was when we balanced the dataset. In theory, a lot of the predictors have a base probability of 0.5, so balancing a dataset should improve performance significantly. However, in practice, we saw no improvement in results after attempting to balance this dataset.\r\n\r\n### Authors and Contributors\r\nIf you have questions or concerns, please message Allen Chen (@allenchen248), Richard Wang (lmetatron), or Jesse Chen (@chenje16).\r\n\r\n### Data\r\nThe data that we used for our project can be found here: TODO: insert Dropbox link to our data here (required by project spec)","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}